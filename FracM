
import math
import torch
from torch.optim.optimizer import Optimizer
import numpy as np
import torch.nn as nn
#import torch.optim as Optimizer

class FraM(Optimizer):


    def __init__(self, params, lr=1e-1, betas=(0, 0), eps=1e-5, weight_decay=1e-4):
        if not 0.0 <= lr:
            raise ValueError("Invalid learning rate: {}".format(lr))
        if not 0.0 <= eps:
            raise ValueError("Invalid epsilon value: {}".format(eps))
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super(FraM, self).__init__(params, defaults)

    def __setstate__(self, state):
        super(FraM, self).__setstate__(state)

    def step(self, closure=None):

        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError('FracM does not support sparse gradients')

                state = self.state[p]

                # State initialization
                if len(state) == 0:
                    state['step'] = 0
                    # Exponential moving average of gradient values
                    state['exp_avg'] = torch.zeros_like(p.data)
                    state['exp1_avg'] = torch.zeros_like(p.data)
                    state['exp2_avg'] = torch.zeros_like(p.data)
                    state['exp3_avg'] = torch.zeros_like(p.data)
                    state['exp4_avg'] = torch.zeros_like(p.data)
                    state['exp5_avg'] = torch.zeros_like(p.data)
                    state['exp6_avg'] = torch.zeros_like(p.data)
                    state['exp7_avg'] = torch.zeros_like(p.data)
                    state['exp8_avg'] = torch.zeros_like(p.data)
                    state['exp9_avg'] = torch.zeros_like(p.data)
                    state['exp10_avg'] = torch.zeros_like(p.data)
                    # Previous gradient
                    state['previous_grad'] = torch.zeros_like(p.data)
                    state['previous1_grad'] = torch.zeros_like(p.data)
                    state['previous2_grad'] = torch.zeros_like(p.data)
                    state['previous3_grad'] = torch.zeros_like(p.data)
                    state['previous4_grad'] = torch.zeros_like(p.data)
                    state['previous5_grad'] = torch.zeros_like(p.data)
                    state['previous6_grad'] = torch.zeros_like(p.data)
                    state['previous7_grad'] = torch.zeros_like(p.data)
                    state['previous8_grad'] = torch.zeros_like(p.data)
                    state['previous9_grad'] = torch.zeros_like(p.data)
                    state['previous10_grad'] = torch.zeros_like(p.data)
                    
                exp_avg, exp1_avg,exp2_avg,exp3_avg,exp4_avg,exp5_avg,exp6_avg,exp7_avg,exp8_avg,exp9_avg,exp10_avg = state['exp_avg'],state['exp1_avg'],state['exp2_avg'],state['exp3_avg'],state['exp4_avg'],state['exp5_avg'],state['exp6_avg'],state['exp7_avg'],state['exp8_avg'],state['exp9_avg'],state['exp10_avg']
                previous_grad, previous1_grad, previous2_grad, previous3_grad, previous4_grad,  previous5_grad, previous6_grad, previous7_grad, previous8_grad, previous9_grad, previous10_grad = state['previous_grad'], state['previous1_grad'], state['previous2_grad'], state['previous3_grad'], state['previous4_grad'], state['previous5_grad'], state['previous6_grad'], state['previous7_grad'], state['previous8_grad'], state['previous9_grad'], state['previous10_grad']
                
                beta1, beta2 = group['betas']

                state['step'] += 1

                if group['weight_decay'] != 0:
                    grad.add_(group['weight_decay'], p.data)

                #M
                #0.9-fractional order first
                exp_avg.add_(1, grad.clone()).add_(1*0.9, exp_avg).add_(-0.48229, previous2_grad).add_(-0.48229*0.9, exp2_avg).add_(-0.0241149, previous4_grad).add_(-0.0241149*0.9, exp4_avg).add_(-0.00884, previous6_grad).add_(-0.00884*0.9, exp6_avg).add_(-0.004642, previous8_grad).add_(-0.004642*0.9, exp8_avg).add_(-0.002878, previous10_grad).add_(-0.002878*0.9, exp10_avg)
                
                #0.5-fractional order second
                # exp_avg.add_(1, grad.clone()).add_(1*0.9, exp_avg).add_(-0.35335, previous2_grad).add_(-0.35335*0.9, exp2_avg).add_(-0.088388, previous4_grad).add_(-0.088388*0.9, exp4_avg).add_(-0.04419, previous6_grad).add_(-0.04419*0.9, exp6_avg).add_(-0.2762, previous8_grad).add_(-0.2762*0.9, exp8_avg)
                
                
                #2.5-fractional order third
                # exp_avg.add_(1, grad.clone()).add_(1*0.9, exp_avg).add_(-0.441941, previous2_grad).add_(-0.441941*0.9, exp2_avg).add_(0.331456, previous4_grad).add_(0.331456*0.9, exp4_avg).add_(-0.055242, previous6_grad).add_(-0.055242*0.9, exp6_avg).add_(-0.006905, previous8_grad).add_(-0.006905*0.9, exp8_avg).add_(-0.00207, previous10_grad).add_(-0.00207*0.9, exp10_avg)
                
                
                #4.2-fractional order four
                # exp_avg.add_(1, grad.clone()).add_(1*0.9, exp_avg).add_( -0.2285195, previous2_grad).add_( -0.2285195*0.9, exp2_avg).add_(0.365631, previous4_grad).add_(0.365631*0.9, exp4_avg).add_(-0.268129, previous6_grad).add_(-0.268129*0.9, exp6_avg).add_(0.0804388, previous8_grad).add_(0.0804388*0.9, exp8_avg).add_( -0.0032175, previous10_grad).add_( -0.0032175*0.9, exp10_avg) 
                
                #0.2-fractional order five
                # exp_avg.add_(1, grad.clone()).add_(1*0.9, exp_avg).add_(-0.17411011, previous2_grad).add_(-0.17411011*0.9, exp2_avg).add_(-0.069644, previous4_grad).add_(-0.069644*0.9, exp4_avg).add_(-0.041786, previous6_grad).add_(-0.041786*0.9, exp6_avg).add_(-0.02925049, previous8_grad).add_(-0.02925049*0.9, exp8_avg)
                
   
           
           
           
           
                
     
                exp10_avg=exp9_avg
                exp9_avg=exp8_avg
                exp8_avg=exp7_avg
                exp7_avg=exp6_avg
                exp6_avg=exp5_avg
                exp5_avg=exp4_avg
                exp4_avg=exp3_avg
                exp3_avg=exp2_avg
                exp2_avg=exp1_avg
                exp1_avg=exp_avg
                
                
                
                previous10_grad = previous9_grad
                previous9_grad = previous8_grad
                previous8_grad = previous7_grad
                previous7_grad = previous6_grad
                previous6_grad = previous5_grad
                previous5_grad = previous4_grad
                previous4_grad = previous3_grad
                previous3_grad = previous2_grad
                previous2_grad = previous1_grad
                previous1_grad = grad.clone()
				
                
             
                exp_avg1 = exp_avg
                
                
                state['exp_avg'] = torch.zeros_like(p.data)

                
             
                step_size = group['lr']
                
                
                p.data.add_(-step_size, exp_avg1)
                
        return loss
